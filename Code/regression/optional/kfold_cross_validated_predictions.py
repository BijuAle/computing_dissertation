import numpy as np
from matplotlib import pyplot as plt,rc

from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeRegressor

rc('font',**{'family':'serif','serif':['Helvetica']})
rc('text', usetex=True)

def get_features_targets(data):
   #n lines, 4 columns
  features = np.zeros((data.shape[0], 4))  
  features[:,0] = data['u'] - data['g']
  features[:,1] = data['g'] - data['r']
  features[:,2] = data['r'] - data['i']
  features[:,3] = data['i'] - data['z']
  targets = data['redshift']
  return (features, targets)

def median_diff(predicted, actual):
  diff = np.median(np.absolute(predicted - actual))
  return diff

def cross_validate_predictions(model, features, targets, k):
  kf = KFold(n_splits=k, shuffle=True)

  # declare an array for predicted redshifts from each iteration
  all_predictions = np.zeros_like(targets)

  for train_indices, test_indices in kf.split(features):
    print (train_indices, len(train_indices),test_indices, len(test_indices))
    # split the data into training and testing
    train_features, test_features = features[train_indices], features[test_indices]
    train_targets, test_targets = targets[train_indices], targets[test_indices]
  
    # fit the model for the current set
    model.fit(train_features, train_targets)

    # predict using the model
    predictions = model.predict(test_features)

    # put the predicted values in the 
    # all_predictions array defined above
    all_predictions[test_indices] = predictions
    
    # note: the line above is filling 
    # multiple disjoint elements of the
    #array at the same time. for example, if:
    #test_indexes is 8 11 23
    #with test_features z8, z11, z23
    #the code does: allpredictions[8,11,23] = z8, z11, z23
    #and hence, once per galaxy   
    
  # return the predictions
  return all_predictions    

if __name__ == "__main__":
  data = np.load('./sdss_galaxy_colors.npy')
  features, targets = get_features_targets(data)

  # initialize model
  dtr = DecisionTreeRegressor(max_depth=19)

  # call cross validation function
  predictions = cross_validate_predictions(dtr, features, targets, 10)

  # calculate and print the rmsd as a sanity check
  diffs = median_diff(predictions, targets)
  print('Median difference: {:.3f}'.format(diffs))

  # plot the results to see how well our model looks
  plt.scatter(targets, predictions, s=0.4)
  plt.xlim((0, targets.max()))
  plt.ylim((0, predictions.max()))
  plt.xlabel('Measured Redshift')
  plt.ylabel('Predicted Redshift')
  plt.grid(b=True, which='major', color='#666666', linestyle='-')
  plt.minorticks_on()
  plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)
  plt.show()


